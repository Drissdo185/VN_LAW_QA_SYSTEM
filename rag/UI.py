import streamlit as st
import json
import time
import tiktoken
from auto_rag import AutoRAG

st.set_page_config(
    page_title="Tr·ª£ L√Ω Lu·∫≠t Giao Th√¥ng Vi·ªát Nam",
    page_icon="üö¶",
    layout="wide"
)

def count_tokens(text, model="gpt-4o-mini"):
    """Count tokens in text using tiktoken"""
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(text)
    return len(tokens)

def initialize_session_state():
    """Initialize session state variables"""
    if "rag" not in st.session_state:
        st.session_state.rag = None
    if "history" not in st.session_state:
        st.session_state.history = []
    if "total_tokens" not in st.session_state:
        st.session_state.total_tokens = 0
    if "is_processing" not in st.session_state:
        st.session_state.is_processing = False

def setup_rag_system():
    """Configure and initialize the RAG system"""
    st.sidebar.title("C·∫•u H√¨nh H·ªá Th·ªëng")
    
    
    
    llm_provider = st.sidebar.radio("LLM Provider", ["openai", "vllm"], index=0)

    if llm_provider == "openai":
        model_name = st.sidebar.selectbox("OpenAI Model", ["gpt-4o-mini"], index=0)
        vllm_api_url = ""
        vllm_model_name = ""
        vllm_max_tokens = 4096
    
    else:
        model_name = ""
        vllm_api_url = st.sidebar.text_input("VLLM API URL", value="http://192.168.100.125:8000/v1/completions")
        vllm_model_name = st.sidebar.selectbox(
        "VLLM Model Name", 
        ["Qwen/Qwen2.5-14B-Instruct-AWQ", "Qwen/Qwen2.5-3B-Instruct"],
        index=0
    )
        vllm_max_tokens = st.sidebar.number_input("Max Tokens", value=4096)
        
    
    
    if st.sidebar.button("Kh·ªüi T·∫°o H·ªá Th·ªëng"):
        with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng RAG..."):
            try:
                st.session_state.rag = AutoRAG(
                    weaviate_host="192.168.100.125",
                    weaviate_port=8080,
                    weaviate_grpc_port=50051,
                    index_name="ND168",
                    embed_model_name="bkai-foundation-models/vietnamese-bi-encoder",
                    embed_cache_folder="/home/drissdo/.cache/huggingface/hub",
                    model_name="gpt-4o-mini",
                    temperature=0.2,
                    top_k=10,
                    alpha=0.5,
                    max_iterations=3,
                    llm_provider="openai",
                    vllm_api_url=vllm_api_url,
                    vllm_model_name=vllm_model_name,
                    vllm_max_tokens=vllm_max_tokens
                )
                st.sidebar.success("H·ªá th·ªëng ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
            except Exception as e:
                st.sidebar.error(f"L·ªói khi kh·ªüi t·∫°o: {str(e)}")
    
    st.sidebar.divider()
    st.sidebar.subheader("Th·ªëng k√™ token")
    st.sidebar.metric("T·ªïng token ƒë√£ s·ª≠ d·ª•ng", st.session_state.total_tokens)
    
    if st.sidebar.button("X√≥a l·ªãch s·ª≠"):
        st.session_state.history = []
        st.session_state.total_tokens = 0

def process_query(question):
    """Process a user query with the RAG system"""
    if st.session_state.rag is None:
        st.error("Vui l√≤ng kh·ªüi t·∫°o h·ªá th·ªëng tr∆∞·ªõc khi s·ª≠ d·ª•ng!")
        return None
    
    input_tokens = count_tokens(question)
    st.session_state.total_tokens += input_tokens
    
    start_time = time.time()
    result = st.session_state.rag.process(question)
    processing_time = time.time() - start_time
    
    answer = result["answer"]
    output_tokens = count_tokens(answer)
    st.session_state.total_tokens += output_tokens
    
    query_record = {
        "question": question,
        "answer": answer,
        "processing_time": processing_time,
        "input_tokens": input_tokens,
        "output_tokens": output_tokens,
        "total_tokens": input_tokens + output_tokens,
        "details": result
    }
    
    st.session_state.history.append(query_record)
    return query_record

def display_results(query_record):
    """Display the results of a processed query"""
    if query_record:
        col1, col2 = st.columns([2, 1])
        
        with col1:
            st.markdown("### C√¢u tr·∫£ l·ªùi")
            st.markdown(query_record["answer"])
            
            with st.expander("Chi ti·∫øt v·ªÅ c√¢u h·ªèi"):
                st.write("**C√¢u h·ªèi g·ªëc:**", query_record["question"])
                st.write("**C√¢u h·ªèi ƒë√£ x·ª≠ l√Ω:**", query_record["details"]["processed_question"])
                st.write("**Lo·∫°i c√¢u h·ªèi:**", query_record["details"].get("question_type", "N/A"))
                
                if "replacements" in query_record["details"]:
                    st.write("**Thay th·∫ø thu·∫≠t ng·ªØ:**")
                    for original, legal in query_record["details"]["replacements"].items():
                        st.write(f"- '{original}' ‚Üí '{legal}'")
        
        with col2:
            st.markdown("### Th·ªëng k√™")
            st.metric("Th·ªùi gian x·ª≠ l√Ω", f"{query_record['processing_time']:.2f}s")
            st.metric("Token ƒë·∫ßu v√†o", query_record["input_tokens"])
            st.metric("Token ƒë·∫ßu ra", query_record["output_tokens"])
            st.metric("T·ªïng token", query_record["total_tokens"])
            
            if "iterations" in query_record["details"]:
                st.write(f"**S·ªë l·∫ßn l·∫∑p:** {query_record['details']['iterations']}")
        
        # Query information section
        st.markdown("### Th√¥ng tin truy v·∫•n")
        if "formatted_query" in query_record["details"]:
            st.write("**Truy v·∫•n ƒë·ªãnh d·∫°ng:**", query_record["details"]["formatted_query"])
        
        # Display iterations
        if "query_history" in query_record["details"]:
            st.markdown("#### Qu√° tr√¨nh l·∫∑p")
            tabs = st.tabs([f"L·∫ßn l·∫∑p {i+1}" for i in range(len(query_record["details"]["query_history"]))])
            
            for i, (tab, query) in enumerate(zip(tabs, query_record["details"]["query_history"])):
                with tab:
                    st.write("**Truy v·∫•n:**", query)
        
        # Display extracted information
        if "context" in query_record["details"] and query_record["details"]["context"]:
            st.markdown("#### Th√¥ng tin ƒë√£ tr√≠ch xu·∫•t")
            st.text_area("N·ªôi dung", query_record["details"]["context"], height=300)

def display_history():
    """Display the history of processed queries"""
    st.subheader("L·ªãch s·ª≠ c√¢u h·ªèi")
    if not st.session_state.history:
        st.info("Ch∆∞a c√≥ c√¢u h·ªèi n√†o ƒë∆∞·ª£c ƒë·∫∑t")
        return
    
    for i, query in enumerate(reversed(st.session_state.history)):
        with st.expander(f"{len(st.session_state.history) - i}. {query['question']}"):
            st.write("**C√¢u tr·∫£ l·ªùi:**")
            st.write(query["answer"])
            st.write(f"**Th·ªùi gian x·ª≠ l√Ω:** {query['processing_time']:.2f}s")
            st.write(f"**Token s·ª≠ d·ª•ng:** {query['total_tokens']}")

def main():
    """Main application function"""
    initialize_session_state()
    
    st.title("Tr·ª£ L√Ω Lu·∫≠t Giao Th√¥ng Vi·ªát Nam üö¶")
    
    setup_rag_system()
    
    st.divider()
    
    question = st.text_area("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n v·ªÅ lu·∫≠t giao th√¥ng:", height=100)
    
    col1, col2 = st.columns([1, 5])
    with col1:
        submit_button = st.button("G·ª≠i c√¢u h·ªèi")
    
    if submit_button and question and not st.session_state.is_processing:
        st.session_state.is_processing = True
        
        with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n..."):
            query_record = process_query(question)
        
        st.session_state.is_processing = False
        display_results(query_record)
    
    st.divider()
    display_history()

if __name__ == "__main__":
    main()